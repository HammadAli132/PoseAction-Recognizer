# ğŸ¥Š PoseAction-Recognizer

PoseAction-Recognizer is a research-driven project for **boxing action recognition** using **computer vision** and **deep learning**.  
The system combines **object detection (YOLOv8)** and **pose estimation (skeleton-based models)** to classify different boxing punches and movements.  

---

## âš ï¸ Project Status

ğŸš§ **This project is currently under active development.**  
The repository structure (especially inside the `/data` directory) may undergo **major updates** as the project evolves.  
Expect changes in preprocessing pipelines, dataset formats, and training workflows as new functionality is added.  

---

## ğŸš€ Project Goals

- Automate **recognition of boxing punches**: jab, cross, hook, uppercut, etc.  
- Leverage **pose estimation (skeletons)** instead of relying only on bounding boxes.  
- Provide a foundation for future systems that can **predict counter-moves** during boxing training.  
- Support both **custom raw video datasets** and **pre-annotated datasets (e.g., Roboflow)**.  
- Offer clean, modular **data preprocessing, training, and visualization tools**.  

---

## ğŸ“‚ Repository Structure

```bash
PoseAction-Recognizer/
â”‚â”€â”€ data/                     # (ignored in .gitignore, must be created locally)
â”‚   â”œâ”€â”€ raw/                  # Raw boxing videos organized by class
â”‚   â”‚   â”œâ”€â”€ jab/video1.mp4
â”‚   â”‚   â”œâ”€â”€ cross/video2.mp4
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ frames/           # Extracted frames from videos
â”‚   â”‚   â”‚   â”œâ”€â”€ jab_video1/frame_000001.jpg
â”‚   â”‚   â”‚   â””â”€â”€ metadata.json # FPS, frame count, etc. for each video
â”‚   â”‚   â””â”€â”€ poses/            # Poses extracted from frames
â”‚   â”‚       â”œâ”€â”€ jab_video1/poses.json
â”‚   â”‚       â”œâ”€â”€ jab_video1/poses.npy
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â””â”€â”€ roboflow/         # Boxing dataset from Roboflow
â”‚   â”‚       â”œâ”€â”€ train/images/...
â”‚   â”‚       â”œâ”€â”€ train/labels/...
â”‚   â”‚       â”œâ”€â”€ valid/images/...
â”‚   â”‚       â”œâ”€â”€ test/images/...
â”‚   â”‚       â””â”€â”€ data.yaml     # YOLO dataset config
â”‚   â”‚
â”‚   â””â”€â”€ poses_datasets/       # Poses extracted from Roboflow dataset
â”‚       â”œâ”€â”€ train/poses.json, poses.npy
â”‚       â”œâ”€â”€ valid/poses.json, poses.npy
â”‚       â””â”€â”€ test/poses.json, poses.npy
â”‚
â”‚â”€â”€ notebooks/                # Jupyter notebooks for prototyping
â”‚
â”‚â”€â”€ scripts/                  # Utility scripts (standalone)
â”‚   â”œâ”€â”€ prepare_data.py       # Extract frames from videos
â”‚   â”œâ”€â”€ extract_poses.py      # Extract poses (from frames or datasets)
â”‚   â”œâ”€â”€ convert_to_coco.py    # (planned) Convert YOLO+poses â†’ COCO for CVAT
â”‚   â”œâ”€â”€ train.sh              # Run training loop
â”‚   â””â”€â”€ evaluate.sh           # Evaluate model performance
â”‚
â”‚â”€â”€ src/                      # Main source code
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ preprocess.py     # CLI menu to run frame/pose extraction
â”‚   â”œâ”€â”€ models/               # ML/DL models (MLP, LSTM, ST-GCN)
â”‚   â”œâ”€â”€ training/             # Training pipelines
â”‚   â””â”€â”€ inference/            # Inference/deployment scripts
â”‚
â”‚â”€â”€ experiments/              # Logs, checkpoints, results
â”‚â”€â”€ config.yaml               # Config file for paths & hyperparams
â”‚â”€â”€ requirements.txt          # Python dependencies
â”‚â”€â”€ README.md                 # You are here
```

---

## âš™ï¸ Functionality

### ğŸ”¹ 1. Data Preprocessing
- **Video â†’ Frames**  
  - Extract frames at a chosen FPS  
  - Save to `data/processed/frames/{action_video}/`  
  - Update `metadata.json` with fps + total frame count  

- **Frames â†’ Poses**  
  - Run YOLOv8-pose on each frame  
  - Save skeletons as `poses.json` (readable) and `poses.npy` (fast loading)  

- **Dataset Images â†’ Poses**  
  - Works with Roboflow dataset (`train/valid/test/images`)  
  - Extracts skeletons for every image  
  - Saves in `data/poses_datasets/{split}/poses.json, poses.npy`  

### ğŸ”¹ 2. Model Training
- **YOLOv8 Detector**  
  - Trains directly on Roboflow dataset with bounding boxes  
  - Evaluates with mAP, precision, recall  

- **Pose Classifier**  
  - Inputs: extracted skeletons (`poses.npy`)  
  - Outputs: punch class (cross, jab, hook, uppercut, etc.)  
  - Models: MLP, RNN/LSTM, or Graph-based (future)  
  - Evaluates with accuracy, F1, confusion matrix  

- **Fusion (planned)**  
  - Combine bounding box detector + pose classifier predictions  

### ğŸ”¹ 3. Visualization
- **Local visualization** (planned): overlay bounding boxes + skeletons on dataset images  
- **CVAT support**:  
  - Convert YOLO labels + pose annotations to **COCO keypoints JSON**  
  - Upload into CVAT â†’ visualize bounding boxes + skeleton overlays together  

---

## ğŸ“Š Dataset Details

### Roboflow Dataset (BoxingHub)
- Classes: `['bag', 'cross', 'hook', 'jab', 'no punch', 'uppercut']`  
- Format: YOLOv8 (images + `.txt` annotations)  
- Provided splits: `train`, `valid`, `test`  
- License: CC BY 4.0  

### Custom Dataset
- Raw boxing videos placed in `data/raw/`  
- Can be automatically segmented into frames and poses  

---

## ğŸ”§ Installation

```bash
# Clone repo
git clone https://github.com/<your-username>/PoseAction-Recognizer.git
cd PoseAction-Recognizer

# Create environment (recommended)
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scriptsctivate      # Windows

# Install dependencies
pip install -r requirements.txt
```

---

## â–¶ï¸ Usage

### Extract Frames from Video
```bash
python -m src.data.preprocess
# Choose option 1
```

### Extract Poses from Frames
```bash
python -m src.data.preprocess
# Choose option 2
```

### Extract Poses from Roboflow Dataset
```bash
python -m src.data.preprocess
# Choose option 3
```

### Train YOLOv8 Detector (on Roboflow dataset)
```bash
yolo detect train data=data/datasets/roboflow/data.yaml model=yolov8s.pt epochs=50 imgsz=640
```

### Train Pose Classifier (planned)
```bash
python -m src.training.train_pose
```

---

## ğŸ“ˆ Evaluation

- **YOLO Detector** â†’ mAP@50-95, per-class precision/recall  
- **Pose Classifier** â†’ Accuracy, F1-score, confusion matrix  
- **Fusion** â†’ Compare individual vs combined models  

---

## âœ… Roadmap
- [x] Video â†’ Frames â†’ Poses pipeline  
- [x] Support Roboflow dataset with images + annotations  
- [ ] Train pose classifier  
- [ ] Add fusion model for joint prediction  
- [ ] Visualization script for dataset inspection  
- [ ] Convert poses+labels â†’ COCO keypoints for CVAT  

---

## ğŸ‘¨â€ğŸ’» Authors
- Hammad Ali & Team â€“ Final Year Project  

---

## ğŸ“œ License
This project is released under the MIT License.  
Dataset (BoxingHub) is under **CC BY 4.0** (see Roboflow dataset page).  
